


import pandas as pd
import os

folder_path = "data/newari_data_archive"

filenames = []
texts = []
for file in os.listdir(folder_path):
    filenames.append(file)
    with open(os.path.join(folder_path, file), "r", encoding="utf-8") as f:
        content = f.read()
        content = content.replace("\n", " ")
        texts.append(content)

corpus = pd.DataFrame({
    "filename": filenames,
    "text": texts
})

corpus.to_csv("data/corpus.csv", index=False)





from pathlib import Path
import pandas as pd

folder_path = Path("data/newari_data_archive")

files = list(folder_path.glob("*"))

corpus = pd.DataFrame({
    "filename": [f.name for f in files],
    # "text": [f.read_text(encoding="utf-8").replace("\n", " ") for f in files]
    "text": [f.read_text(encoding="utf-8") for f in files]
})

corpus.to_csv("data/corpus.csv", index=False)





import pandas as pd
import re


corpus = pd.read_csv("data/corpus_trec.csv")


_DEVANAGARI_PATTERN = re.compile(r'[\u0900-\u0963\u0966-\u097F]+')

with open("data/stopwords.txt", "r", encoding="utf-8") as f:
    _STOPWORDS = set(line.strip() for line in f)

def tokenize(text):
    # extract words but ignore । and ॥
    words = _DEVANAGARI_PATTERN.findall(text)
    normalized = [word.replace("\u0940", "\u093F").replace("\u0942", "\u0941") for word in words]
    return [word for word in normalized if word not in _STOPWORDS]





from collections import Counter

def compute_term_frequency(documents):
    term_frequency = Counter()
    for document in documents:
        term_frequency.update(tokenize(document))

    # return term_frequency.most_common()
    return sorted(term_frequency.items(), key=lambda x: x[0])

# term_frequencies = compute_term_frequency(corpus['text'].tolist())

# df = pd.DataFrame(term_frequencies, columns=['term', 'frequency'])
# df.to_csv("data/term_frequency_alphabetically.csv", index=False)


# df = pd.read_csv("data/term_frequency.csv")
# df.shape


# df = df[df['frequency'] > 3000]
# df.shape


# import seaborn as sns
# import matplotlib.pyplot as plt
#
# sns.histplot(df, x="frequency", bins=10, kde=False)
# plt.title("Histogram of term frequency")
# plt.show()


# df.to_csv("data/stopwords.csv", index=False)





from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    tokenizer=tokenize,
    use_idf=True,
    sublinear_tf=True,
)

tfidf_matrix = vectorizer.fit_transform(corpus["text"].tolist())
tfidf_matrix.shape


print(type(tfidf_matrix))





def build_inverted_index(tfidf_matrix, vectorizer):
    from collections import defaultdict
    coo = tfidf_matrix.tocoo()
    inverted_index = defaultdict(list)
    terms = vectorizer.get_feature_names_out()

    for docid, termid, weight in zip(coo.row, coo.col, coo.data):
        inverted_index[terms[termid]].append((docid, weight))

    for postings in inverted_index.values():
        postings.sort(key=lambda x: x[1], reverse=True)

    return inverted_index

inverted_index = build_inverted_index(tfidf_matrix, vectorizer)





from sklearn.metrics.pairwise import cosine_similarity

def search(query, tfidf_matrix, vectorizer, top_k=10):
    query_vector = vectorizer.transform([query])
    similarities = cosine_similarity(query_vector, tfidf_matrix)[0]
    top_indices = similarities.argsort()[-top_k:][::-1]
    return [(idx, similarities[idx]) for idx in top_indices]





import pickle
import scipy.sparse

# Save the vectorizer
with open('models/vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)

# Save the TF-IDF matrix
scipy.sparse.save_npz('models/tfidf_matrix.npz', tfidf_matrix)





output = search("नेपाल भारतया", tfidf_matrix, vectorizer, top_k=3)

ids = corpus['DocID']
for docid, score in output:
    print("Document id: " + str(ids[docid]) + " with score " + str(score) + "\n")
    print(corpus['text'].iloc[docid])
    print("\n----------------\n")





# trec_utils.py

def save_trec_run(qid, doc_id_list, score_list, system_name, filename, mode='a'):
    """
    Saves a list of retrieved documents into the standard TREC run format.

    Format: QID ITERATION_ID DOCID RANK SCORE SYSTEM_NAME
    """
    with open(filename, mode) as f:
        # ITERATION_ID is typically QID-A (or just 'Q0') but we'll use QID-SystemName for uniqueness
        iteration_id = f"{qid}-{system_name}"
        
        # Ensure lists are the same length
        num_results = len(doc_id_list)
        
        for rank, (docid, score) in enumerate(zip(doc_id_list, score_list)):
            # TREC ranks start at 1
            trec_rank = rank + 1
            
            # Format: QID Q0 DOCID RANK SCORE SYSTEM_NAME
            # Note: The standard format often uses 'Q0' for the second column, 
            # but using the system name here helps with internal tracking.
            line = f"{qid} {system_name} {docid} {trec_rank} {score:.6f}\n"
            f.write(line)

def read_topics_xml(filename):
    """
    A simple function to parse QID and Title from your topics.xml file.
    (Requires the 'xml.etree.ElementTree' module, which is standard)
    """
    import xml.etree.ElementTree as ET
    
    queries = []
    tree = ET.parse(filename)
    root = tree.getroot()
    
    for topic in root.findall('top'):
        qid = topic.find('num').text.strip().replace('Q', '').replace('q', '').strip() # Clean QID
        title = topic.find('title').text.strip()
        
        # Pad QID to four digits (e.g., '1' -> '0001') for consistency
        queries.append({
            'qid': f"Q{qid.zfill(4)}",
            'title': title
        })
    return queries





from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict

def tokenize(text):
    # extract words but ignore । and ॥
    words = _DEVANAGARI_PATTERN.findall(text)
    normalized = [word.replace("\u0940", "\u093F").replace("\u0942", "\u0941") for word in words]
    return [word for word in normalized if word not in _STOPWORDS]

def compute_tfidf(corpus):
    vectorizer = TfidfVectorizer( tokenizer=tokenize, token_pattern=None, use_idf=True, sublinear_tf=True)

    tfidf_matrix = vectorizer.fit_transform(corpus["text"].tolist())

    return tfidf_matrix, vectorizer

def search(query, tfidf_matrix, vectorizer, top_k=10):
    query_vector = vectorizer.transform([query])
    similarities = cosine_similarity(query_vector, tfidf_matrix)[0]
    top_indices = similarities.argsort()[-top_k:][::-1]
    return [(idx, similarities[idx]) for idx in top_indices]


# tfidf_runner.py
import pandas as pd

# --- Configuration ---
TOP_K = 50
TOPICS_FILE = "data/newa_queries.trec"
RUN_FILE = "data/tfidf_run.txt"
CORPUS_FILE = 'data/corpus_trec.csv'
SYSTEM_NAME = 'TFIDF'

def run_tfidf_retrieval():
    print(f"--- Running {SYSTEM_NAME} Model ---")
    
    # 1. Load Data and Compute TF-IDF
    corpus = pd.read_csv(CORPUS_FILE)
    # The DocID column must be correctly named in your corpus.csv
    doc_ids = corpus['DocID'].tolist() 
    
    tfidf_matrix, vectorizer = compute_tfidf(corpus)
    
    # 2. Load Queries
    queries = read_topics_xml(TOPICS_FILE)
    
    # 3. Clear previous run file and run queries
    with open(RUN_FILE, 'w') as f: # Clear the file by opening in write mode ('w')
        pass

    for query in queries:
        qid = query['qid']
        title = query['title']
        
        # VSM Search (Using your existing utility structure)
        # Assuming newautils.vsm.search returns a list of tuples (DocID, Score)
        try:
            results = search(title, tfidf_matrix, vectorizer, top_k=TOP_K)
            
            # Extract DocIDs and Scores from the search result tuples
            doc_id_list = [doc_ids[r[0]] for r in results]
            score_list = [r[1] for r in results]
            
            # Save to TREC format (append mode)
            save_trec_run(qid, doc_id_list, score_list, SYSTEM_NAME, RUN_FILE, mode='a')
            print(f"Processed Query {qid} with {len(results)} results.")
            
        except Exception as e:
            print(f"Error processing {qid} ('{title}'): {e}")
    
    print(f"TF-IDF retrieval complete. Results saved to {RUN_FILE}")

run_tfidf_retrieval()





# muril_runner.py
import pandas as pd
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel

# --- Configuration ---
TOP_K = 50
TOPICS_FILE = "data/newa_queries.trec"
RUN_FILE = "data/muril_run.txt"
CORPUS_FILE = 'data/corpus_trec.csv'
SYSTEM_NAME = 'MURIL'
MODEL_NAME = "sundeepdwd/muril-mlm-newa-finetuned"

def get_mean_pooling(model_output, attention_mask):
    """Mean pooling to get a single vector representation for the sequence."""
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    return sum_embeddings / sum_mask

def run_muril_retrieval():
    print(f"--- Running {SYSTEM_NAME} Model ---")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    # 1. Load Model and Data
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModel.from_pretrained(MODEL_NAME)
    
    model.to(device)

    corpus = pd.read_csv(CORPUS_FILE)
    doc_ids = corpus['DocID'].tolist() 
    doc_texts = corpus['text'].tolist()

    # NOTE: This step is memory intensive. For 80,000+ docs, pre-calculate embeddings once.
    print("Computing document embeddings (can take time for large corpus)...")
    doc_embeddings = []
    
    # Batch processing is recommended for 80k+ documents, but for simplicity:
    for text in doc_texts:
        tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
        tokens.to(device)
        with torch.no_grad():
            output = model(**tokens)
        doc_embeddings.append(get_mean_pooling(output, tokens['attention_mask']))
        
    all_doc_embeddings = torch.cat(doc_embeddings)
    
    # 2. Load Queries
    queries = read_topics_xml(TOPICS_FILE)
    
    # 3. Clear previous run file and run queries
    with open(RUN_FILE, 'w') as f:
        pass

    for query in queries:
        qid = query['qid']
        title = query['title'] # Use the title as the query text

        # 4. Compute Query Embedding
        query_tokens = tokenizer(title, return_tensors='pt', truncation=True, padding=True)
        query_tokens.to(device)
        with torch.no_grad():
            query_output = model(**query_tokens)
        query_embedding = get_mean_pooling(query_output, query_tokens['attention_mask'])
        
        # 5. Calculate Cosine Similarity
        # Compare the query embedding to all document embeddings
        # The result is a 1D tensor of scores
        scores = F.cosine_similarity(query_embedding, all_doc_embeddings)
        
        # 6. Sort and Select Top K
        top_scores, top_indices = torch.topk(scores, k=min(TOP_K, len(scores)))
        
        # Retrieve the corresponding DocIDs
        top_doc_ids = [doc_ids[i.item()] for i in top_indices]
        
        # Save to TREC format
        save_trec_run(qid, top_doc_ids, top_scores.tolist(), SYSTEM_NAME, RUN_FILE, mode='a')
        print(f"Processed Query {qid} with {len(top_doc_ids)} results.")
        
    print(f"MuRIL retrieval complete. Results saved to {RUN_FILE}")

if __name__ == "__main__":
    run_muril_retrieval()





import pandas as pd

df = pd.read_csv("data/corpus.csv")
output_filename = "data/corpus_trec.csv"
prefix =  "NWA"
num_docs = len(df)
    
# Determine how many digits are needed for zero-padding (e.g., 80123 -> 5 digits)
padding_length = len(str(num_docs))

print(f"Found {num_docs} documents. Using {padding_length} digits for DocID padding.")

# 3. Generate the sequential, padded DocIDs
# Create a list of numbers (1, 2, 3, ...)
doc_numbers = range(1, num_docs + 1)

# Format the numbers with zero-padding and add the prefix
# Example: 1 -> '00001', 80123 -> '80123'
docids = [f'{prefix}{num:0{padding_length}d}' for num in doc_numbers]

# 4. Insert the new DocID column
# We insert it as the first column for clarity
df.insert(0, 'DocID', docids)

# 5. Save the new, clean corpus file
df.to_csv(output_filename, index=False)







import pandas as pd
# Read the corpus CSV
corpus = pd.read_csv('data/corpus_trec.csv')

# Randomly sample 100 documents
sample_size = 1000
queryset = corpus[['DocID', 'text']].sample(n=sample_size, random_state=45)

# Save to queryset.csv
queryset.to_csv('data/queryset.csv', index=False)





import pandas as pd

df = pd.read_csv("data/querydoc.csv")
df.head()
output_filename = "data/queries.csv"
prefix = "Q"
num_docs = len(df)
    
# Determine how many digits are needed for zero-padding (e.g., 80123 -> 5 digits)
padding_length = len(str(num_docs))

print(f"Found {num_docs} documents. Using {padding_length} digits for DocID padding.")

# 3. Generate the sequential, padded DocIDs
# Create a list of numbers (1, 2, 3, ...)
doc_numbers = range(1, num_docs + 1)

# Format the numbers with zero-padding and add the prefix
# Example: 1 -> '00001', 80123 -> '80123'
docids = [f'{prefix}{num:0{padding_length}d}' for num in doc_numbers]

# 4. Insert the new DocID column
# We insert it as the first column for clarity
df.insert(0, 'QueryID', docids)

# 5. Save the new, clean corpus file
df[['QueryID', 'Query']].to_csv(output_filename, index=False)






import pandas as pd

queries = pd.read_csv("data/queries.csv")
querydoc = pd.read_csv("data/querydoc.csv")

df = pd.DataFrame()
df['QueryID'] = queries['QueryID']
df['relevant docs'] = querydoc['DocID']

df.to_csv('data/groundtruth.csv', index=False)




{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "queries = pd.read_csv(\"data/queries.csv\")\n",
    "docs = pd.read_csv(\"data/corpus_trec.csv\")\n",
    "ground = pd.read_csv(\"data/groundtruth.csv\")\n",
    "\n",
    "# Merge ground truth with query text\n",
    "data = ground.merge(queries, on=\"QueryID\", how=\"left\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c66dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(docs, left_on=\"relevant docs\", right_on=\"DocID\", how=\"left\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2834279",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_pairs = data[[\"Query\", \"text\"]]\n",
    "query_doc_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e138bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_pairs.to_csv(\"data/query_doc_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85293c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "query_doc_pairs = pd.read_csv(\"data/query_doc_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example: split into 500 train and 500 test\n",
    "train_pairs, test_pairs = train_test_split(\n",
    "    query_doc_pairs,\n",
    "    train_size=500,\n",
    "    test_size=500,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_pairs)}, Test size: {len(test_pairs)}\")\n",
    "train_pairs.to_csv(\"data/train_pairs.csv\", index=False)\n",
    "test_pairs.to_csv(\"data/test_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09dd1d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 train queries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "test_pairs = pd.read_csv(\"data/train_pairs.csv\")      # file with Query, text\n",
    "queries = pd.read_csv(\"data/queries.csv\")            # file with QueryID, Query\n",
    "\n",
    "queries = queries.drop_duplicates(\"Query\")\n",
    "test_queries = queries.merge(test_pairs, on=\"Query\", how=\"inner\")[[\"QueryID\", \"Query\"]]\n",
    "\n",
    "# Save to a new file\n",
    "test_queries.to_csv(\"data/train_queries.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(test_queries)} train queries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3269265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "train_pairs = pd.read_csv(\"data/train_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2887cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = list(zip(train_pairs[\"Query\"], train_pairs[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7914fb-7564-410f-b670-2ca06faa6152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n",
      "DataLoader created.\n",
      "Loss function and optimizer set.\n",
      "Starting training...\n",
      "Epoch 1: loss = 0.6636\n",
      "Epoch 2: loss = 0.4940\n",
      "Epoch 3: loss = 0.1072\n",
      "Epoch 4: loss = 0.0173\n",
      "Epoch 5: loss = 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('muril-contrastive-newari/tokenizer_config.json',\n",
       " 'muril-contrastive-newari/special_tokens_map.json',\n",
       " 'muril-contrastive-newari/vocab.txt',\n",
       " 'muril-contrastive-newari/added_tokens.json',\n",
       " 'muril-contrastive-newari/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "MODEL_NAME = \"google/muril-base-cased\"\n",
    "\n",
    "# 1. Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "print(\"Model and tokenizer loaded.\")\n",
    "\n",
    "# 2. Dataset\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs  # [(text1, text2), ...]\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "train_data = ContrastiveDataset(train_pairs)\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, random_seed=42)\n",
    "print(\"DataLoader created.\")\n",
    "\n",
    "# 3. Mean pooling helper\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return (token_embeddings * input_mask_expanded).sum(1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# 4. Contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, emb1, emb2):\n",
    "        # Normalize\n",
    "        emb1 = nn.functional.normalize(emb1, p=2, dim=1)\n",
    "        emb2 = nn.functional.normalize(emb2, p=2, dim=1)\n",
    "\n",
    "        logits = torch.matmul(emb1, emb2.T) / self.temperature\n",
    "        labels = torch.arange(len(emb1)).to(emb1.device)\n",
    "        loss_i = nn.functional.cross_entropy(logits, labels)\n",
    "        loss_j = nn.functional.cross_entropy(logits.T, labels)\n",
    "        return (loss_i + loss_j) / 2\n",
    "\n",
    "loss_fn = ContrastiveLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "print(\"Loss function and optimizer set.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 5. Training loop\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        t1 = [x[0] for x in batch]\n",
    "        t2 = [x[1] for x in batch]\n",
    "\n",
    "        inputs1 = tokenizer(list(t1), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        inputs2 = tokenizer(list(t2), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "        out1 = model(**inputs1)\n",
    "        out2 = model(**inputs2)\n",
    "\n",
    "        emb1 = mean_pooling(out1, inputs1['attention_mask'])\n",
    "        emb2 = mean_pooling(out2, inputs2['attention_mask'])\n",
    "\n",
    "        loss = loss_fn(emb1, emb2)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss = {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 6. Save fine-tuned model\n",
    "model.save_pretrained(\"muril-contrastive-newari\")\n",
    "tokenizer.save_pretrained(\"muril-contrastive-newari\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ade30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at sundeepdwd/muril-mlm-newa-finetuned and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n",
      "DataLoader created.\n",
      "Loss function and optimizer set.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "MODEL_NAME = \"sundeepdwd/muril-mlm-newa-finetuned\"\n",
    "\n",
    "# 1. Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "print(\"Model and tokenizer loaded.\")\n",
    "\n",
    "# 2. Dataset\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs  # [(text1, text2), ...]\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "train_data = ContrastiveDataset(train_pairs)\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "print(\"DataLoader created.\")\n",
    "\n",
    "# 3. Mean pooling helper\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return (token_embeddings * input_mask_expanded).sum(1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# 4. Contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, emb1, emb2):\n",
    "        # Normalize\n",
    "        emb1 = nn.functional.normalize(emb1, p=2, dim=1)\n",
    "        emb2 = nn.functional.normalize(emb2, p=2, dim=1)\n",
    "\n",
    "        logits = torch.matmul(emb1, emb2.T) / self.temperature\n",
    "        labels = torch.arange(len(emb1)).to(emb1.device)\n",
    "        loss_i = nn.functional.cross_entropy(logits, labels)\n",
    "        loss_j = nn.functional.cross_entropy(logits.T, labels)\n",
    "        return (loss_i + loss_j) / 2\n",
    "\n",
    "loss_fn = ContrastiveLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "print(\"Loss function and optimizer set.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 5. Training loop\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        t1 = [x[0] for x in batch]\n",
    "        t2 = [x[1] for x in batch]\n",
    "\n",
    "        inputs1 = tokenizer(list(t1), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        inputs2 = tokenizer(list(t2), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "        out1 = model(**inputs1)\n",
    "        out2 = model(**inputs2)\n",
    "\n",
    "        emb1 = mean_pooling(out1, inputs1['attention_mask'])\n",
    "        emb2 = mean_pooling(out2, inputs2['attention_mask'])\n",
    "\n",
    "        loss = loss_fn(emb1, emb2)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss = {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 6. Save fine-tuned model\n",
    "model.save_pretrained(\"muril-pretrained-contrastive-newari\")\n",
    "tokenizer.save_pretrained(\"muril-pretrained-contrastive-newari\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e51b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
